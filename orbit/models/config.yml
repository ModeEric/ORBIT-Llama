task: llm-sft
base_model: meta-llama/Meta-Llama-3-8B
project_name: a-project-name
log: tensorboard
backend: local

data:
  path: /path/to/files
  train_split: train
  valid_split: null
  chat_template: null
  column_mapping:
    text_column: text

params:
  block_size: 2048
  model_max_length: 8192
  epochs: 1
  batch_size: 1
  lr: 2e-5
  peft: false
  quantization: bf16
  padding: right
  optimizer: paged_adamw_8bit
  scheduler: linear
  gradient_accumulation: 8
  use_flash_attn2: true

hub:
  username: username
  token: hf_token
  push_to_hub: true

# Default training configurations for ORBIT models

# Astronomy domain configuration
astronomy:
  learning_rate: 2.0e-5
  num_train_epochs: 3
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 8
  warmup_steps: 100
  weight_decay: 0.01
  max_seq_length: 2048
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  fp16: true
  logging_steps: 10
  save_strategy: "epoch"
  save_total_limit: 3
  report_to: "tensorboard"

# Law domain configuration
law:
  learning_rate: 1.0e-5
  num_train_epochs: 4
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 16
  warmup_steps: 200
  weight_decay: 0.01
  max_seq_length: 4096
  lora_r: 32
  lora_alpha: 64
  lora_dropout: 0.1
  fp16: true
  logging_steps: 10
  save_strategy: "epoch"
  save_total_limit: 3
  report_to: "tensorboard"

# Medical domain configuration
medical:
  learning_rate: 1.5e-5
  num_train_epochs: 3
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 16
  warmup_steps: 150
  weight_decay: 0.01
  max_seq_length: 4096
  lora_r: 32
  lora_alpha: 64
  lora_dropout: 0.1
  fp16: true
  logging_steps: 10
  save_strategy: "epoch"
  save_total_limit: 3
  report_to: "tensorboard"

# Custom domain default configuration
custom:
  learning_rate: 2.0e-5
  num_train_epochs: 3
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 8
  warmup_steps: 100
  weight_decay: 0.01
  max_seq_length: 2048
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  fp16: true
  logging_steps: 10
  save_strategy: "epoch"
  save_total_limit: 3
  report_to: "tensorboard"